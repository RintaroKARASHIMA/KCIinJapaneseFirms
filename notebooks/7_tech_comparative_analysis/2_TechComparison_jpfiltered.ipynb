{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../src')\n",
    "from ecomplexity import ecomplexity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import matplotlib.ticker as ptick\n",
    "import networkx as nx\n",
    "import networkx.algorithms.bipartite as bip\n",
    "\n",
    "# plt.rcParams['font.family'] = 'Meiryo'\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# 小数点以下 桁数 6\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initial_condition\n",
    "from process import weight\n",
    "from visualize import rank as vr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir, filter_dir, output_dir\n",
    "data_dir = '../../data/interim/internal/filtered_before_agg/'\n",
    "filter_dir = '../../data/interim/internal/filter_after_agg/'\n",
    "output_dir = '../../data/interim/internal/filtered_after_agg/'\n",
    "ex_dir = '../../data/processed/external/schmoch/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期条件\n",
    "ar = initial_condition.AR\n",
    "year_style = initial_condition.YEAR_STYLE\n",
    "\n",
    "year_start = initial_condition.YEAR_START\n",
    "year_end = initial_condition.YEAR_END\n",
    "year_range = initial_condition.YEAR_RANGE\n",
    "\n",
    "extract_population = initial_condition.EXTRACT_POPULATION\n",
    "top_p_or_num = initial_condition.TOP_P_OR_NUM\n",
    "# top_p_or_num = ('p', 100)\n",
    "region_corporation = initial_condition.REGION_CORPORATION\n",
    "# region_corporation = 'right_person_addr'\n",
    "applicant_weight = initial_condition.APPLICANT_WEIGHT\n",
    "\n",
    "classification = initial_condition.CLASSIFICATION\n",
    "class_weight = initial_condition.CLASS_WEIGHT\n",
    "\n",
    "filter_condition = f'{ar}_{year_style}_{extract_population}_reg_num_top_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}'\n",
    "input_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全体\n",
    "all_df = pd.read_csv(f'{data_dir}japan.csv', \n",
    "                     encoding='utf-8', \n",
    "                     sep=',', \n",
    "                     usecols=['reg_num', \n",
    "                              region_corporation, 'right_person_addr',\n",
    "                              f'{ar}_{year_style}', \n",
    "                              f'{classification}'], \n",
    "                     dtype={'reg_num': str, \n",
    "                            region_corporation: str, \n",
    "                            f'{ar}_{year_style}': np.int64, \n",
    "                            f'{classification}': str})\n",
    "\n",
    "all_df = all_df[all_df[f'{ar}_{year_style}'].isin(range(year_start, year_end+1))]\\\n",
    "               .drop_duplicates()\\\n",
    "# display(all_df.head())\n",
    "\n",
    "\n",
    "# 各期間\n",
    "sep_year_df_dict = {}\n",
    "\n",
    "for year in range(year_start, year_end+1, year_range):\n",
    "    sep_year_df_dict[f'{year}-{year+year_range-1}'] = all_df[all_df[f'{ar}_{year_style}'].isin(range(year, year+year_range))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特許分類による重みづけ\n",
    "# 全体\n",
    "if class_weight == 'fraction':\n",
    "    all_df = weight.by_classification(all_df, region_corporation, classification)\n",
    "elif class_weight == 'duplication':\n",
    "    all_df['class_weight'] = 1\n",
    "all_df[f'{ar}_{year_style}_period'] = f'{year_start}-{year_end}'\n",
    "\n",
    "\n",
    "# 共同出願の重みづけ\n",
    "# 全体\n",
    "if applicant_weight == 'fraction':\n",
    "    all_df = weight.by_applicant(all_df, region_corporation)\n",
    "elif applicant_weight == 'duplication':\n",
    "    all_df['applicant_weight'] = 1\n",
    "all_df[f'{ar}_{year_style}_period'] = f'{year_start}-{year_end}'\n",
    "all_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィルタリング\n",
    "reg_num_filter_df = pd.read_csv(f'{filter_dir}{filter_condition}.csv',\n",
    "                                encoding='utf-8',\n",
    "                                sep=',', \n",
    "                                usecols=[f'{ar}_{year_style}_period', region_corporation],\n",
    "                                dtype=str)\n",
    "reg_num_filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(all_df, reg_num_filter_df, on=[f'{ar}_{year_style}_period', 'right_person_name'], how='inner')\n",
    "all_reg_num_df = df.copy()\n",
    "all_reg_num_df['reg_num'] = 1 / all_reg_num_df['class_weight'] / all_reg_num_df['applicant_weight']\n",
    "all_reg_num_df = all_reg_num_df.groupby([f'{ar}_{year_style}_period', 'right_person_addr', classification])[['reg_num']]\\\n",
    "                               .sum().reset_index(drop=False)\\\n",
    "                               .sort_values(['reg_num'], ascending=[False])\n",
    "all_reg_num_df\n",
    "# sep_year_reg_num_df_dict = sep_year_df_dict.copy()\n",
    "# for period, sep_year_reg_num_df in sep_year_reg_num_df_dict.items():\n",
    "#     sep_year_reg_num_df['reg_num'] = 1 / sep_year_reg_num_df['class_weight'] / sep_year_reg_num_df['applicant_weight']\n",
    "#     sep_year_reg_num_df = sep_year_reg_num_df.groupby([f'{ar}_{year_style}_period', region_corporation, classification])[['reg_num']]\\\n",
    "#                                              .sum().reset_index(drop=False)\\\n",
    "#                                              .sort_values(['reg_num'], ascending=[False])\n",
    "#     sep_year_reg_num_df_dict[period] = sep_year_reg_num_df\n",
    "# sep_year_reg_num_df = pd.concat([sep_year_reg_num_df for sep_year_reg_num_df in sep_year_reg_num_df_dict.values()], axis='index', ignore_index=True)\n",
    "# sep_year_reg_num_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_cols = {'time':f'{ar}_{year_style}_period', 'loc':'right_person_addr', 'prod':classification, 'val':'reg_num'}\n",
    "rename_col_dict = {'eci':'kci', 'pci':'tci'}\n",
    "col_order_list = [f'{ar}_{year_style}_period', 'right_person_addr', classification, 'reg_num', 'rca', 'mcp', 'diversity', 'ubiquity', 'kci', 'tci']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kh_ki(c_df, classification, n=19):\n",
    "    kh1_ki1_df = pd.merge(c_df.copy(), \n",
    "                        c_df[c_df['mcp']==1].groupby(['right_person_addr'])[['ubiquity']].sum().reset_index(drop=False).copy().rename(columns={'ubiquity':'kh_1'}), \n",
    "                        on=['right_person_addr'], how='left')\n",
    "    kh1_ki1_df = pd.merge(kh1_ki1_df.copy(), \n",
    "                        c_df[c_df['mcp']==1].groupby([classification])[['diversity']].sum().reset_index(drop=False).copy().rename(columns={'diversity':'ki_1'}), \n",
    "                        on=[classification], how='left')\n",
    "    kh1_ki1_df['kh_1'] = kh1_ki1_df['kh_1'] / kh1_ki1_df['diversity']\n",
    "    kh1_ki1_df['ki_1'] = kh1_ki1_df['ki_1'] / kh1_ki1_df['ubiquity']\n",
    "    kh_ki_df = kh1_ki1_df.copy()\n",
    "    for i in range(n):\n",
    "        kh_ki_df = pd.merge(kh_ki_df, \n",
    "                            kh_ki_df[kh_ki_df['mcp']==1].groupby(['right_person_addr'])[[f'ki_{i+1}']].sum().reset_index(drop=False).copy()\\\n",
    "                                        .rename(columns={f'ki_{i+1}':f'kh_{i+2}'}), \n",
    "                            on=['right_person_addr'], how='left')\n",
    "        kh_ki_df = pd.merge(kh_ki_df, \n",
    "                            kh_ki_df[kh_ki_df['mcp']==1].groupby([classification])[[f'kh_{i+1}']].sum().reset_index(drop=False).copy()\\\n",
    "                                        .rename(columns={f'kh_{i+1}':f'ki_{i+2}'}), \n",
    "                            on=[classification], how='left')\n",
    "        kh_ki_df[f'kh_{i+2}'] = kh_ki_df[f'kh_{i+2}'] / kh_ki_df['diversity']\n",
    "        kh_ki_df[f'ki_{i+2}'] = kh_ki_df[f'ki_{i+2}'] / kh_ki_df['ubiquity']\n",
    "    return kh_ki_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = ecomplexity(all_reg_num_df,\n",
    "                   cols_input = trade_cols, \n",
    "                   rca_mcp_threshold = 1)\n",
    "# c_out_df = c_df.copy()\n",
    "c_df = c_df[c_df['reg_num'] > 0]\\\n",
    "           .rename(columns=rename_col_dict)\\\n",
    "           [col_order_list]\n",
    "c_df = pd.concat([kh_ki(c_df[c_df[f'{ar}_{year_style}_period'] == period], classification) for period in c_df[f'{ar}_{year_style}_period'].unique()], \n",
    "                 axis='index', \n",
    "                 ignore_index=True)\n",
    "c_df[classification] = c_df[classification].astype(int)\n",
    "schmoch_df = pd.read_csv(f'{ex_dir}35.csv', \n",
    "                         encoding='utf-8', \n",
    "                         sep=',', \n",
    "                         usecols=['Field_number', 'Field_en']\n",
    "                         ).drop_duplicates()\n",
    "\n",
    "c_df = pd.merge(c_df, \n",
    "                schmoch_df, \n",
    "                left_on=[classification], \n",
    "                right_on=['Field_number'], \n",
    "                how='left').drop(columns=['Field_number', classification]).rename(columns={'Field_en': classification})\n",
    "# c_df = c_df[[f'{ar}_{year_style}_period', classification, 'tci']]\n",
    "pre_df = c_df[[f'{ar}_{year_style}_period', classification, 'tci']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_df = pd.read_csv(f'../../data/processed/internal/tech/{input_condition}.csv', \n",
    "                    encoding='utf-8',\n",
    "                    sep=','\n",
    "                    )[[f'{ar}_{year_style}_period', classification, 'tci']]\n",
    "co_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_df = pd.read_csv('../../data/processed/external/abroad/eu.csv', \n",
    "                    encoding='utf-8', \n",
    "                    sep=',')\n",
    "eu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_co_df = pd.merge(pre_df.rename(columns={'tci':'pre_tci'}), co_df.rename(columns={'tci':'co_tci'}), on=[f'{ar}_{year_style}_period', classification], how='inner')\n",
    "pre_co_df = pd.merge(pre_co_df, eu_df[['schmoch35', 'schmoch5']], on='schmoch35', how='left')\n",
    "pre_co_df['schmoch5'] = pre_co_df['schmoch5'].replace('Mechanical engineering', 'Mechanical engineering, machinery')\n",
    "pre_co_df['schmoch5'] = pre_co_df['schmoch5'].replace('Chemistry', 'Chemistry, pharmaceuticals')\n",
    "pre_co_df['co_tci'] = (pre_co_df['co_tci'] - pre_co_df['co_tci'].min()) / (pre_co_df['co_tci'].max() - pre_co_df['co_tci'].min()) * 100\n",
    "pre_co_df['pre_tci'] = (pre_co_df['pre_tci'] - pre_co_df['pre_tci'].min()) / (pre_co_df['pre_tci'].max() - pre_co_df['pre_tci'].min()) * 100\n",
    "pre_co_df = pre_co_df.drop_duplicates().reset_index(drop=True)\n",
    "pre_co_df['schmoch5'] = np.where(pre_co_df['schmoch35']=='Machine tools', \n",
    "                                 'Mechanical engineering, machinery', \n",
    "                                 np.where(pre_co_df['schmoch35']=='Analysis of biological materials', \n",
    "                                          'Instruments', \n",
    "                                          pre_co_df['schmoch5']))\n",
    "pre_co_df['co_tci_rank'] = pre_co_df['co_tci'].rank(ascending=False, method='min')\n",
    "pre_co_df['pre_tci_rank'] = pre_co_df['pre_tci'].rank(ascending=False, method='min')\n",
    "pre_co_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = all_df[~(all_df['right_person_name'].isin(df['right_person_name']))].copy()\n",
    "sample_df[classification] = sample_df[classification].astype(int)\n",
    "sample_df = pd.merge(sample_df, schmoch_df, left_on=[classification], right_on=['Field_number'], how='left').drop(columns=['Field_number', classification]).rename(columns={'Field_en': classification})\n",
    "sample_df = pd.merge(sample_df, eu_df[['schmoch35', 'schmoch5']], on='schmoch35', how='left')\\\n",
    "                [[classification, 'schmoch5', 'reg_num', 'right_person_name', 'right_person_addr']]\n",
    "sample_df['schmoch5'] = sample_df['schmoch5'].replace('Mechanical engineering', 'Mechanical engineering, machinery')\n",
    "sample_df['schmoch5'] = sample_df['schmoch5'].replace('Chemistry', 'Chemistry, pharmaceuticals')\n",
    "sample_df['schmoch5'] = np.where(sample_df['schmoch35']=='Machine tools', \n",
    "                                 'Mechanical engineering, machinery', \n",
    "                                 np.where(sample_df['schmoch35']=='Analysis of biological materials', \n",
    "                                          'Instruments', \n",
    "                                          sample_df['schmoch5']))\n",
    "print(sample_df['schmoch35'].unique())\n",
    "sample_df[sample_df['schmoch35']=='Digital communication'].groupby([classification, 'right_person_addr'], as_index=False).nunique().sort_values(['right_person_name'], ascending=[False])\n",
    "sample_df.groupby([classification], as_index=False).nunique().sort_values(['right_person_name'], ascending=[False])\n",
    "sample_df.groupby([classification, 'right_person_addr'], as_index=False).nunique().sort_values(['reg_num', classification], ascending=False).drop_duplicates(subset=[classification], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df[(c_df[classification]=='Civil engineering')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = c_df.groupby([classification])[['right_person_addr']].nunique().sort_values(['right_person_addr'], ascending=[False])\n",
    "b = c_df[[classification, 'ubiquity', 'tci']].drop_duplicates().sort_values(['tci'], ascending=[False])\n",
    "c = pd.merge(a, b, on=[classification], how='inner').sort_values(['tci'], ascending=[False])\n",
    "c['decrease_rate'] = 1 - (c['ubiquity'] / c['right_person_addr'])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_co_df.drop_duplicates().reset_index(drop=True).sort_values(['co_tci'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "tech_color = {\n",
    "        'Chemistry, pharmaceuticals': 'red',\n",
    "        'Electrical engineering': 'blue',\n",
    "        'Instruments': 'green', \n",
    "        'Mechanical engineering, machinery': 'orange',\n",
    "        'Other fields': 'gray'\n",
    "    }\n",
    "combi_dict = {  # ind: [x, y, title, xlabel, ylabel, legend_loc]\n",
    "    1: [\"co_tci\", \"pre_tci\", \"relation between the TCIs in Japanese Corporations and Prefectures\", \"Corporations（period：1981-2010 fiscal year）\", \"Prefectures（period：1981-2010 fiscal year）\", \"center\", ],\n",
    "    2: [\"co_tci_rank\", \"pre_tci_rank\", \"relation between the TCI rankings in Japanese Corporations and Prefectures\", \"Corporations（period：1981-2010 fiscal year）\", \"Prefectures（period：1981-2010 fiscal year）\", \"center\", ],\n",
    "    # 2: [\"TCI_rank_jp\", \"TCI_rank_eu\", \"relation between the TCIs in Japanese corporation and EU regions\", \"Japanese Corporations ranking（period：1981-2010 fiscal year）\", \"EU Regions ranking（period：1985-2009 year）\", \"center\", ],\n",
    "    # 2: [\"reg_num_jp\", \"reg_num_eu\", \"corr between the patent amounts in Japan and EU\", \"Japan（period：1981-2010 fiscal year）\", \"EU（period：1985-2009 year）\", \"center\", ],\n",
    "    # 3: [\"reg_num_jp\", \"TCI_jp\", \"relation between the patent counts and the TCIs in Japan\", \"Patent Counts\", \"TCIs\", \"center left\", ],\n",
    "    # 4: [\"TCI_jp\", \"reg_num_jp\", \"relation between the patent counts and the TCIs in Japan\", \"TCIs\", \"Patent Counts\", \"center left\", ],\n",
    "    # 5: [\"reg_num_eu\", \"TCI_eu\", \"corr between the patent amounts in EU and TCI in EU\", \"EU（period：1985-2009 year）\", \"EU（period：1985-2009 year）\", \"center\", ],\n",
    "    # 2: [\"TCI_eu\", \"TCI_jp\", \"corr between the TCIs in Japan and EU\", \"EU（period：1985-2009 year）\", \"Japan（period：1981-2010 fiscal year）\", \"center\", ],\n",
    "}\n",
    "plt.rcParams['font.size'] = 24\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "for i, combi in combi_dict.items():\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    period = f\"{year_start}-{year_end}\"\n",
    "    corr_num = round(pre_co_df[combi[0]].corr(pre_co_df[combi[1]]), 3)\n",
    "    print(period, corr_num)\n",
    "    # ax.scatter(pre_co_df[combi[0]], pre_co_df[combi[1]],\n",
    "    #            s=20, alpha=0.8, color=\"black\", )\n",
    "    # if i == 4:\n",
    "    ax.axvline(x=pre_co_df[combi[0]].mean(), color=\"gray\", linestyle=\"--\", )\n",
    "    ax.axhline(y=pre_co_df[combi[1]].mean(), color=\"gray\", linestyle=\"--\", )\n",
    "    ax.set_title(combi[2]+'(corr=' + r\"$\\bf{\" + str(corr_num)+ \"}$\" +')\\n')\n",
    "    if combi[0] in [\"reg_num\"]: ax.set_xscale(\"log\")\n",
    "    if combi[1] in [\"reg_num\"]: ax.set_yscale(\"log\")\n",
    "    x_min = pre_co_df[combi[0]].min()\n",
    "    x_2smallest = (pre_co_df[combi[0]].nsmallest(2).iloc[1])\n",
    "    y_2smallest = (pre_co_df[combi[1]].nsmallest(2).iloc[1])\n",
    "    head_df = pre_co_df.head(5)\n",
    "    between_df = pre_co_df.iloc[5:len(pre_co_df)-5, :]\n",
    "    tail_df = pre_co_df.tail(5)\n",
    "    if i != 5:\n",
    "        # display(pre_co_df)\n",
    "        # for i, row in head_df.iterrows():\n",
    "        #     ax.text(row[combi[0]], row[combi[1]], f'{i+1} {row[\"schmoch35\"]}', fontsize=18, color=\"red\")\n",
    "        #     ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"red\")\n",
    "        # for i, row in between_df.iterrows():\n",
    "        #     ax.text(row[combi[0]], row[combi[1]], f'{i+1} {row[\"schmoch35\"]}', fontsize=15, color=\"black\")\n",
    "        #     ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"black\")\n",
    "        # for i, row in tail_df.iterrows():\n",
    "        #     ax.text(row[combi[0]], row[combi[1]], f'{i+1} {row[\"schmoch35\"]}', fontsize=18, color=\"blue\", )\n",
    "        #     ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"blue\")\n",
    "        # for i, row in head_df.iterrows():\n",
    "        #     ax.text(row[combi[0]], row[combi[1]], f'{i+1} {row[\"schmoch35\"]}', fontsize=18, color=\"red\")\n",
    "            \n",
    "            # if i == 4: ax.scatter(row[combi[0]], row[combi[1]], s=40, color=tech_color[row['schmoch5']], label=row['schmoch5'])\n",
    "            # else: ax.scatter(row[combi[0]], row[combi[1]], s=40, color=tech_color[row['schmoch5']])\n",
    "        # for i, row in between_df.iterrows():\n",
    "        #     # ax.text(row[combi[0]], row[combi[1]], i+1, fontsize=15, color=\"black\")\n",
    "        #     if i == 7: ax.scatter(row[combi[0]], row[combi[1]], s=40, color=tech_color[row['schmoch5']], label=row['schmoch5'])\n",
    "        #     else: ax.scatter(row[combi[0]], row[combi[1]], s=40, color=tech_color[row['schmoch5']])\n",
    "            \n",
    "        # for i, row in tail_df.iterrows():\n",
    "        #     # ax.text(row[combi[0]], row[combi[1]], i+1, fontsize=18, color=\"blue\")\n",
    "        #     ax.scatter(row[combi[0]], row[combi[1]], s=40, color=\"blue\", label=f'{i+1} {row[\"schmoch35\"]}')\n",
    "        for tech_color_key in tech_color.keys():\n",
    "            ax.scatter(pre_co_df[pre_co_df['schmoch5']==tech_color_key][combi[0]], pre_co_df[pre_co_df['schmoch5']==tech_color_key][combi[1]], \n",
    "                       color=tech_color[tech_color_key], label=tech_color_key, \n",
    "                       s=60)\n",
    "        # for i, row in pre_co_df.iterrows():\n",
    "        #     ax.text(row[combi[0]], row[combi[1]], f'{i+1} {row[\"schmoch35\"]}', fontsize=18, color=\"black\")\n",
    "        # for ind, row in head_df.iterrows():\n",
    "        #     if ind == 1: ax.text(row[combi[0]]+1, row[combi[1]]-2, f'\\n{ind+1} {row[\"schmoch35\"]}', fontsize=20, color=tech_color[row['schmoch5']])\n",
    "        #     else: ax.text(row[combi[0]]+1, row[combi[1]]-1, f'{ind+1} {row[\"schmoch35\"]}', fontsize=20, color=tech_color[row['schmoch5']])\n",
    "    # elif i == 2:\n",
    "    #     for i, row in head_df.iterrows():\n",
    "    #         ax.text(row[combi[0]], row[combi[1]], i+1, fontsize=18, color=\"red\")\n",
    "    #         ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"red\")\n",
    "    #     for i, row in between_df.iterrows():\n",
    "    #         ax.text(row[combi[0]], row[combi[1]], i+1, fontsize=15, color=\"black\")\n",
    "    #         ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"black\")\n",
    "    #     for i, row in tail_df.iterrows():\n",
    "    #         ax.text(row[combi[0]], row[combi[1]], i+1, fontsize=18, color=\"blue\", )\n",
    "    #         ax.scatter(row[combi[0]], row[combi[1]], s=20, color=\"blue\")\n",
    "    ax.set_ylabel(combi[4])\n",
    "    ax.set_xlabel(combi[3])\n",
    "    # ax.set_xscale('log')\n",
    "    ax.legend(loc=combi[5], fontsize=20, bbox_to_anchor=(1.65, 0.5), borderaxespad=0, prop={'weight': 'bold'})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economic_complexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
