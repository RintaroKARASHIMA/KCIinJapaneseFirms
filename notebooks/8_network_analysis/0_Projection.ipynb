{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top></a>\n",
    "\n",
    "# **目次**\n",
    "\n",
    "<b>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=#modules, style='font-size: xx-large'>1. モジュールインポート</a>\n",
    "            <ul>※サードパーティライブラリ>>>自作モジュール>>>（ここまで本ipynb外）>>>自作関数（本ipynb内）</ul>\n",
    "        </summary>\n",
    "    </details>\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        <a href=#data , style='font-size: xx-large'>2. オリジナルデータインポート</a>\n",
    "    </summary>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary>\n",
    "        <a href='#all', style='font-size: xx-large'>3. 全体</a>\n",
    "    </summary>\n",
    "    <table></table>\n",
    "</details>\n",
    "    \n",
    "<details>\n",
    "    <summary>\n",
    "        <a href='#sepyear', style='font-size: xx-large'>4. 期間ごと</a>\n",
    "    </summary>\n",
    "</details>\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=modules></a>\n",
    "\n",
    "## **1. モジュールインポート**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../src')\n",
    "from ecomplexity import ecomplexity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "import matplotlib.ticker as ptick\n",
    "import networkx as nx\n",
    "import networkx.algorithms.bipartite as bip\n",
    "\n",
    "plt.rcParams['font.family'] = 'Meiryo'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# 小数点以下 桁数 6\n",
    "# pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作モジュールインポート\n",
    "import initial_condition\n",
    "from process import weight\n",
    "from visualize import rank as vr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccdf(diversity_col: list):\n",
    "        freq_array = np.array(np.bincount(diversity_col))\n",
    "        p_list = []\n",
    "        cumsum = 0.0\n",
    "        s = float(freq_array.sum())\n",
    "        for freq in freq_array:\n",
    "            if freq != 0:\n",
    "                cumsum += freq / s\n",
    "                p_list.append(cumsum)\n",
    "            else:\n",
    "                p_list.append(1.0)\n",
    "                \n",
    "        ccdf_array = 1 - np.array(p_list)\n",
    "        if ccdf_array[0] == 0:\n",
    "            ccdf_array[0] = 1.0\n",
    "        return ccdf_array\n",
    "\n",
    "color_list = ['red']+[\n",
    "    'red', 'green', 'blue', 'yellow', 'orange', 'purple', 'pink', 'brown',\n",
    "    'grey', 'violet', 'indigo', 'turquoise', 'gold', 'lime', 'coral',\n",
    "    'navy', 'skyblue', 'tomato', 'olive', 'cyan', 'darkred', 'darkgreen',\n",
    "    'darkblue', 'darkorange', 'darkviolet', 'deeppink', 'firebrick', 'darkcyan',\n",
    "    'darkturquoise', 'darkslategray', 'darkgoldenrod', 'mediumblue', 'mediumseagreen',\n",
    "    'mediumpurple', 'mediumvioletred', 'midnightblue', 'saddlebrown', 'seagreen',\n",
    "    'sienna', 'steelblue'\n",
    "    ][10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccdf(target_array:np.array):\n",
    "    freq_array = np.array(np.bincount(target_array))\n",
    "    p_list = []\n",
    "    cumsum = 0.0\n",
    "    s = float(freq_array.sum())\n",
    "    for freq in freq_array:\n",
    "        if freq != 0:\n",
    "            cumsum +=freq / s\n",
    "            p_list.apend(cumsum)\n",
    "        else:\n",
    "            p_list.append(1.0)\n",
    "    ccdf_array = 1 - np.arrau(p_list)\n",
    "    if ccdf_array[0] == 0: ccdf_array[0] = 1.0\n",
    "    return ccdf_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def descript_network(graph_df:pd.DataFrame, \n",
    "                     hr_col:str='right_person_name', \n",
    "                     class_col:str='ipc_class', \n",
    "                     segment_col:str='1981-2010'\n",
    "                     ):\n",
    "    # ネットワークの特性を調べる\n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(graph_df[hr_col].unique(), bipartite=0)\n",
    "    BG.add_nodes_from(graph_df[class_col].unique(), bipartite=1)\n",
    "    BG.add_edges_from(list(zip(graph_df[hr_col], graph_df[class_col])))\n",
    "    \n",
    "    if nx.is_connected(BG): print('連結性ある')\n",
    "    else: print('連結性ない')\n",
    "    \n",
    "    # 特許権者ノード\n",
    "    hr_nodes = {n for n, d in BG.nodes(data=True) if d['bipartite'] == 0}\n",
    "    hr_degree_array = ccdf(dict(bip.degrees(BG, hr_nodes)[1]).values())\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 特許分類ノード\n",
    "    ipc_nodes = set(BG) - hr_nodes\n",
    "    ipc_degree_array = ccdf(dict(bip.degrees(BG, ipc_nodes)[1]).values())\n",
    "    \n",
    "    # G = nx.from_pandas_edgelist(graph_df, 'source', 'target', edge_attr=True)\n",
    "    # print(nx.info(G))\n",
    "    # print('平均最短距離:', nx.average_shortest_path_length(G))\n",
    "    # print('平均クラスタリング係数:', nx.average_clustering(G))\n",
    "    # print('直径:', nx.diameter(G))\n",
    "    # print('半径:', nx.radius(G))\n",
    "    # print('次数中心性:', nx.degree_centrality(G))\n",
    "    # print('媒介中心性:', nx.betweenness_centrality(G))\n",
    "    # print('固有ベクトル中心性:', nx.eigenvector_centrality(G))\n",
    "    # print('PageRank:', nx.pagerank(G))\n",
    "    # print('次数分布:', nx.degree_histogram(G))\n",
    "    res_dict = {\n",
    "                'network':BG}\n",
    "    \n",
    "    return BG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir, ex_dir, output_dir\n",
    "data_dir = '../../data/processed/internal/graph/'\n",
    "ex_dir = '../../data/processed/external/'\n",
    "output_dir = '../../output/figures/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期条件\n",
    "ar = initial_condition.AR\n",
    "year_style = initial_condition.YEAR_STYLE\n",
    "\n",
    "year_start = initial_condition.YEAR_START\n",
    "year_end = initial_condition.YEAR_END\n",
    "year_range = initial_condition.YEAR_RANGE\n",
    "\n",
    "extract_population = initial_condition.EXTRACT_POPULATION\n",
    "top_p_or_num = initial_condition.TOP_P_OR_NUM\n",
    "region_corporation = initial_condition.REGION_CORPORATION\n",
    "applicant_weight = initial_condition.APPLICANT_WEIGHT\n",
    "\n",
    "classification = initial_condition.CLASSIFICATION\n",
    "class_weight = initial_condition.CLASS_WEIGHT\n",
    "\n",
    "color_list = initial_condition.COLOR_LIST\n",
    "\n",
    "input_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n",
    "fig_name_base = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}.png'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_df = pd.read_csv(data_dir + f'{input_condition}_edge.csv', \n",
    "                       encoding='utf-8', \n",
    "                       sep=','\n",
    "                       )\n",
    "node_df = pd.read_csv(data_dir + f'{input_condition}_node.csv', \n",
    "                       encoding='utf-8', \n",
    "                       sep=','\n",
    "                       )\n",
    "# edge_df\n",
    "# graph_dict = {}\n",
    "# for s in graph_df['segment'].unique():\n",
    "       \n",
    "#        graph_df = graph_df[graph_df['segment'] == '1981-2010']\n",
    "# graph_df.groupby('right_person_name')[['mcp']].sum().sort_values('mcp', ascending=False).head(10)\n",
    "graph_df = pd.merge(edge_df, node_df, left_on='Source', right_on='node_id', how='left')\\\n",
    "                   [['Target', 'label']]\\\n",
    "                   .rename(columns={'label':region_corporation})\n",
    "graph_df = pd.merge(graph_df, node_df, left_on='Target', right_on='node_id', how='left')\\\n",
    "                     [['label', region_corporation]]\\\n",
    "                        .rename(columns={'label':classification})\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schmoch_df = pd.read_csv(ex_dir + 'schmoch/35.csv', encoding='utf-8', sep=',', usecols=['Field_en', 'schmoch5'])\\\n",
    "                .rename(columns={'Field_en':'schmoch35', 'schmoch5':'schmoch5'})\\\n",
    "                .drop_duplicates(ignore_index=True)\n",
    "schmoch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BG = nx.Graph()\n",
    "BG.add_nodes_from(graph_df[region_corporation].unique(), bipartite=0)\n",
    "BG.add_nodes_from(graph_df[classification].unique(), bipartite=1)\n",
    "BG.add_edges_from(list(zip(graph_df[region_corporation], graph_df[classification])))\n",
    "\n",
    "if nx.is_connected(BG): print('連結性ある')\n",
    "else: print('連結性ない')\n",
    "\n",
    "# 特許権者ノード\n",
    "hr_nodes = {n for n, d in BG.nodes(data=True) if d['bipartite'] == 0}\n",
    "# hr_degree_array = ccdf(dict(bip.degrees(BG, hr_nodes)[1]).values())\n",
    "hr_nodes\n",
    "\n",
    "\n",
    "# 特許分類ノード\n",
    "ipc_nodes = set(BG) - hr_nodes\n",
    "ipc_nodes\n",
    "# ipc_degree_array = ccdf(dict(bip.degrees(BG, ipc_nodes)[1]).values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>先頭に戻る</a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=#description></a>\n",
    "\n",
    "## **1. 記述統計**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特許権者ノードの密度\n",
    "print(round(bip.density(BG, hr_nodes), 3))\n",
    "\n",
    "# 特許分類ノードの密度\n",
    "print(round(bip.density(BG, ipc_nodes), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特許権者同士のつながり\n",
    "hr_G = bip.projected_graph(BG, hr_nodes)\n",
    "hr_degree_dict = dict(hr_G.degree())\n",
    "\n",
    "print(max(hr_degree_dict, key=hr_degree_dict.get))\n",
    "# hr_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特許分類同士のつながり\n",
    "ipc_G = bip.weighted_projected_graph(BG, ipc_nodes)\n",
    "nx.to_pandas_edgelist(ipc_G)\n",
    "# ipc_degree_dict = dict(ipc_G.degree())\n",
    "# for n, w in dict(bip.degrees(BG, ipc_nodes)[1]).items():\n",
    "#     ipc_G.nodes[n]['weight'] = w\n",
    "# print(max(ipc_degree_dict, key=ipc_degree_dict.get))\n",
    "# ipc_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "# Step 1: Load and prepare data\n",
    "data = graph_df.copy()\n",
    "# Step 2: Calculate k_c,0 and k_p,0\n",
    "k_c_0 = data.groupby('right_person_name')['schmoch35'].count().to_dict()\n",
    "k_p_0 = data.groupby('schmoch35')['right_person_name'].count().to_dict()\n",
    "\n",
    "# Step 3: Construct the bipartite graph\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(data['right_person_name'], bipartite=0)\n",
    "B.add_nodes_from(data['schmoch35'], bipartite=1)\n",
    "edges = [(row['right_person_name'], row['schmoch35']) for idx, row in data.iterrows()]\n",
    "B.add_edges_from(edges)\n",
    "\n",
    "# Step 4: Initialize the product-product matrix M_pp with zeros\n",
    "countries = list(data['right_person_name'].unique())\n",
    "products = list(data['schmoch35'].unique())\n",
    "M_pp = np.zeros((len(products), len(products)))\n",
    "\n",
    "# Create dictionaries for quick lookup\n",
    "country_index = {country: i for i, country in enumerate(countries)}\n",
    "product_index = {product: i for i, product in enumerate(products)}\n",
    "\n",
    "# Create adjacency matrix M_cp\n",
    "M_cp = np.zeros((len(countries), len(products)))\n",
    "for idx, row in data.iterrows():\n",
    "    M_cp[country_index[row['right_person_name']], product_index[row['schmoch35']]] = 1\n",
    "\n",
    "# Step 5: Calculate M_pp'\n",
    "for p in products:\n",
    "    for p_prime in products:\n",
    "        if p != p_prime:\n",
    "            for c in countries:\n",
    "                if M_cp[country_index[c], product_index[p]] and M_cp[country_index[c], product_index[p_prime]]:\n",
    "                    M_pp[product_index[p], product_index[p_prime]] += (M_cp[country_index[c], product_index[p]] * M_cp[country_index[c], product_index[p_prime]]) / (k_p_0[p] * k_c_0[c])\n",
    "\n",
    "# Step 6: Normalize the matrix\n",
    "np.fill_diagonal(M_pp, 0)\n",
    "mean_M_pp = np.mean(M_pp)\n",
    "std_M_pp = np.std(M_pp)\n",
    "M_pp_normalized = (M_pp - mean_M_pp) / std_M_pp\n",
    "\n",
    "print('Normalized Product-Product Matrix (M_pp):')\n",
    "print(M_pp_normalized)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(M_pp_normalized)\n",
    "eigenvectors = np.real(eigenvectors)\n",
    "second_largest_index = eigenvalues.argsort()[-1]\n",
    "second_largest_eigenvector = eigenvectors[:, second_largest_index]\n",
    "print('2番目に大きい固有値に対応する固有ベクトル:', second_largest_eigenvector)\n",
    "print(second_largest_index)\n",
    "TCI_dict = {}\n",
    "for k, v in product_index.items():\n",
    "    TCI_dict[k] = second_largest_eigenvector[v]\n",
    "    print(k, second_largest_eigenvector[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_cp = np.random.default_rng().integers(0, 1, (20, 10), endpoint=True)\n",
    "\n",
    "k_c_0 = np.sum(M_cp, axis=1)\n",
    "k_p_0 = np.sum(M_cp, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the product-product matrix M_pp with zeros\n",
    "print('Enter the number of countries and products:')\n",
    "n_countries = int(input('countries: '))\n",
    "n_products = int(input('products: '))\n",
    "M_pp = np.zeros((n_products, n_products))\n",
    "\n",
    "# Create adjacency matrix M_cp\n",
    "M_cp = np.random.default_rng().integers(0, 1, (n_countries, n_products), endpoint=True)\n",
    "\n",
    "k_c_0 = np.sum(M_cp, axis=1)  # countries\n",
    "k_p_0 = np.sum(M_cp, axis=0)  # products\n",
    "# M_cp = pd.read_csv('../data/bipertite_graph.csv', sep=',').to_numpy()\n",
    "\n",
    "# Step 2: Calculate M_pp'\n",
    "for p in range(n_products):\n",
    "    for p_prime in range(n_products):\n",
    "        if p != p_prime:\n",
    "            for c in range(n_countries):\n",
    "                if M_cp[c, p] and M_cp[c, p_prime]:\n",
    "                    M_pp[p, p_prime] += (M_cp[c, p] * M_cp[c, p_prime]) / (k_p_0[p] * k_c_0[c])\n",
    "\n",
    "# Step 3: Normalize the matrix\n",
    "np.fill_diagonal(M_pp, 0)\n",
    "mean_M_pp = np.mean(M_pp)\n",
    "std_M_pp = np.std(M_pp)\n",
    "M_pp_normalized = (M_pp - mean_M_pp) / std_M_pp\n",
    "\n",
    "print('Normalized Product-Product Matrix (M_pp):')\n",
    "print(M_pp_normalized)\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(M_pp_normalized)\n",
    "eigenvectors = np.real(eigenvectors)\n",
    "second_largest_index = eigenvalues.argsort()[-1]\n",
    "second_largest_eigenvector = eigenvectors[:, second_largest_index]\n",
    "TCI = (second_largest_eigenvector - np.min(second_largest_eigenvector)) / (np.max(second_largest_eigenvector) - np.min(second_largest_eigenvector))\n",
    "for i, tci in enumerate(TCI):\n",
    "    print(f'Product {i + 1}: {tci}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(TCI_dict, key=TCI_dict.get)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_edge_df = pd.DataFrame(M_pp*100).reset_index(drop=False).melt(id_vars='index')\\\n",
    "                        .rename(columns={'index':'source', 'variable':'target', 'value':'weight'})\n",
    "projected_node_df = pd.DataFrame.from_dict(product_index, orient='index').reset_index(drop=False)\\\n",
    "                        .rename(columns={0:'node_id', 'index':'label'})[['node_id', 'label']]\n",
    "weight_df = pd.DataFrame.from_dict(dict(bip.degrees(BG, ipc_nodes)[1]), orient='index')\\\n",
    "                        .reset_index(drop=False).rename(columns={0:'weight', 'index':'label'})[['label', 'weight']]\n",
    "projected_node_df = pd.merge(projected_node_df, weight_df, on='label', how='left')\n",
    "projected_node_df = pd.merge(projected_node_df, schmoch_df, left_on='label', right_on='schmoch35', how='left')\\\n",
    "                        [['node_id', 'label', 'weight', 'schmoch5']]\\\n",
    "                        .rename(columns={'schmoch5':'category'})\n",
    "\n",
    "projected_edge_df.to_csv(data_dir + f'{input_condition}_projected_edge.csv',\n",
    "                            encoding='utf-8', \n",
    "                            sep=',', \n",
    "                            index=False)\n",
    "\n",
    "projected_node_df.to_csv(data_dir + f'{input_condition}_projected_node.csv', \n",
    "                          encoding='utf-8', \n",
    "                          sep=',', \n",
    "                          index=False)\n",
    "\n",
    "# projected_node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_fixed_edge_df = pd.read_csv(data_dir + f'{input_condition}_projected_edge.csv', \n",
    "                                      encoding='utf-8', \n",
    "                                      sep=','\n",
    "                                      )\n",
    "projected_fixed_edge_df = projected_fixed_edge_df.query('source != target').copy()\n",
    "projected_fixed_edge_df['normarized_weight'] = (projected_edge_df['weight'] - projected_edge_df['weight'].min())/(projected_edge_df['weight'].max() - projected_edge_df['weight'].min())\n",
    "# projected_fixed_edge_df['normarized_weight'].plot(kind='hist', bins=int(np.log2(len(projected_fixed_edge_df))+1), alpha=0.5)\n",
    "print(*projected_fixed_edge_df[round(projected_fixed_edge_df['normarized_weight'], 4) == round(projected_fixed_edge_df['normarized_weight'].quantile(0.9), 4)]['weight'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_fixed_edge_df['normarized_weight'].median(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_node_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economic_complexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
