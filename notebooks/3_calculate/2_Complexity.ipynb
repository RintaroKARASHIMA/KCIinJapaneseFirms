{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=top></a>\n",
    "\n",
    "# **目次**\n",
    "\n",
    "<b>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=\"#modules\", style=\"font-size: xx-large\">1. モジュールインポート</a>\n",
    "            <ul>※サードパーティライブラリ>>>自作モジュール>>>（ここまで本ipynb外）>>>自作関数（本ipynb内）</ul>\n",
    "        </summary>\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=\"#data\", style=\"font-size: xx-large\">2. オリジナルデータインポート</a>\n",
    "        </summary>\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=\"#patentcount\", style=\"font-size: xx-large\">3. 特許数</a>\n",
    "        </summary>\n",
    "        <table></table>\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=\"#calculateindicator\", style=\"font-size: xx-large\">4. 各指標</a>\n",
    "        </summary>\n",
    "    </details>\n",
    "    <details>\n",
    "        <summary>\n",
    "            <a href=\"#output\", style=\"font-size: xx-large\">5. ファイルに出力</a>\n",
    "        </summary>\n",
    "    </details>\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=modules></a>\n",
    "\n",
    "## **1. モジュールインポート**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "\n",
    "# 小数点以下 桁数 6\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initial_condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir, output_dir, ex_dir\n",
    "data_dir = '../../data/interim/internal/filtered_after_agg/'\n",
    "output_dir = '../../data/processed/internal/'\n",
    "ex_dir = '../../data/processed/external/schmoch/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期条件\n",
    "ar = initial_condition.AR\n",
    "year_style = initial_condition.YEAR_STYLE\n",
    "\n",
    "year_start = initial_condition.YEAR_START\n",
    "year_end = initial_condition.YEAR_END\n",
    "year_range = initial_condition.YEAR_RANGE\n",
    "\n",
    "extract_population = initial_condition.EXTRACT_POPULATION\n",
    "top_p_or_num = initial_condition.TOP_P_OR_NUM\n",
    "# top_p_or_num = ('p', 100)\n",
    "region_corporation = initial_condition.REGION_CORPORATION\n",
    "# region_corporation = 'right_person_addr'\n",
    "applicant_weight = initial_condition.APPLICANT_WEIGHT\n",
    "\n",
    "# classification = initial_condition.CLASSIFICATION\n",
    "classification = 'ipc3'\n",
    "class_weight = initial_condition.CLASS_WEIGHT\n",
    "\n",
    "input_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n",
    "output_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kh_ki(c_df, classification, n=19):\n",
    "    kh1_ki1_df = pd.merge(c_df.copy(), \n",
    "                        c_df[c_df['mcp']==1].groupby([region_corporation])[['ubiquity']].sum().reset_index(drop=False).copy().rename(columns={'ubiquity':'kh_1'}), \n",
    "                        on=[region_corporation], how='left')\n",
    "    kh1_ki1_df = pd.merge(kh1_ki1_df.copy(), \n",
    "                        c_df[c_df['mcp']==1].groupby([classification])[['diversity']].sum().reset_index(drop=False).copy().rename(columns={'diversity':'ki_1'}), \n",
    "                        on=[classification], how='left')\n",
    "    kh1_ki1_df['kh_1'] = kh1_ki1_df['kh_1'] / kh1_ki1_df['diversity']\n",
    "    kh1_ki1_df['ki_1'] = kh1_ki1_df['ki_1'] / kh1_ki1_df['ubiquity']\n",
    "    kh_ki_df = kh1_ki1_df.copy()\n",
    "    for i in range(n):\n",
    "        kh_ki_df = pd.merge(kh_ki_df, \n",
    "                            kh_ki_df[kh_ki_df['mcp']==1].groupby([region_corporation])[[f'ki_{i+1}']].sum().reset_index(drop=False).copy()\\\n",
    "                                        .rename(columns={f'ki_{i+1}':f'kh_{i+2}'}), \n",
    "                            on=[region_corporation], how='left')\n",
    "        kh_ki_df = pd.merge(kh_ki_df, \n",
    "                            kh_ki_df[kh_ki_df['mcp']==1].groupby([classification])[[f'kh_{i+1}']].sum().reset_index(drop=False).copy()\\\n",
    "                                        .rename(columns={f'kh_{i+1}':f'ki_{i+2}'}), \n",
    "                            on=[classification], how='left')\n",
    "        kh_ki_df[f'kh_{i+2}'] = kh_ki_df[f'kh_{i+2}'] / kh_ki_df['diversity']\n",
    "        kh_ki_df[f'ki_{i+2}'] = kh_ki_df[f'ki_{i+2}'] / kh_ki_df['ubiquity']\n",
    "    return kh_ki_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=data></a>\n",
    "\n",
    "## **2. オリジナルデータインポート**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schmoch_df = pd.read_csv(f'{ex_dir}35.csv', \n",
    "                         encoding='utf-8', \n",
    "                         sep=',', \n",
    "                        #  usecols=['Field_number', 'Field_en']\n",
    "                         ).drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_num_top_df = pd.read_csv(f'{data_dir}{input_condition}.csv', \n",
    "                             encoding='utf-8',\n",
    "                             sep=',')\n",
    "reg_num_top_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_num_top_df[region_corporation].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../../data/interim/internal/filtered_before_agg/japan.csv')\\\n",
    "    .query('1981 <= app_nendo <= 2010')\\\n",
    "    .query(f'right_person_addr in {list(reg_num_top_df[region_corporation].unique())}')\\\n",
    "    ['reg_num'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# こちらは指定された行数に対してエクセルの列インデックスのようにアルファベットを使用してIDを生成する問題です。\n",
    "# 1から26はAからZ、その後はAA, AB, ...と続きます。\n",
    "\n",
    "def excel_column_name(n):\n",
    "    name = []\n",
    "    while n > 0:\n",
    "        n, remainder = divmod(n - 1, 26)\n",
    "        name.append(chr(65 + remainder))\n",
    "    return ''.join(reversed(name))\n",
    "\n",
    "# 最後のIDを求める\n",
    "last_id = excel_column_name(1938)\n",
    "last_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>先頭に戻る</a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=calculateindicator></a>\n",
    "\n",
    "## **4. 各指標**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_cols = {'time':f'{ar}_{year_style}_period', 'loc':region_corporation, 'prod':classification, 'val':'reg_num'}\n",
    "rename_col_dict = {'eci':'kci', 'pci':'tci'}\n",
    "col_order_list = [f'{ar}_{year_style}_period', region_corporation, classification, 'reg_num', 'rca', 'mcp', 'diversity', 'ubiquity', 'kci', 'tci']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df = ecomplexity(reg_num_top_df,\n",
    "                   cols_input = trade_cols, \n",
    "                   rca_mcp_threshold = 1)\n",
    "# prox_df = proximity(c_df, trade_cols)\n",
    "# c_out_df = c_df.copy()\n",
    "print(c_df.columns)\n",
    "c_df = c_df[c_df['reg_num'] > 0]\\\n",
    "           .rename(columns=rename_col_dict)\\\n",
    "           [col_order_list]\n",
    "c_df = pd.concat([kh_ki(c_df[c_df[f'{ar}_{year_style}_period'] == period], classification) for period in c_df[f'{ar}_{year_style}_period'].unique()], \n",
    "                 axis='index', \n",
    "                 ignore_index=True)\n",
    "\n",
    "# for segment in c_df[f'{ar}_{year_style}_period'].unique():\n",
    "#     display(c_df[c_df[f'{ar}_{year_style}_period'] == segment].head())\n",
    "#     display(c_df[c_df[f'{ar}_{year_style}_period'] == segment].describe())\n",
    "#     print(c_df[c_df[f'{ar}_{year_style}_period'] == segment].info())\n",
    "#     print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">先頭に戻る</a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=output></a>\n",
    "\n",
    "## **5. ファイルに出力**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=rightperson></a>\n",
    "\n",
    "### **5.1. 特許権者**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "right_person_df = pd.merge(c_df.groupby([f'{ar}_{year_style}_period', region_corporation])[['reg_num']].sum().reset_index(drop=False), \n",
    "                           c_df.groupby([f'{ar}_{year_style}_period', region_corporation])[[classification]].nunique().reset_index(drop=False), \n",
    "                           on=[f'{ar}_{year_style}_period', region_corporation], \n",
    "                           how='inner')\n",
    "right_person_df = pd.merge(right_person_df, \n",
    "                           c_df[[f'{ar}_{year_style}_period', region_corporation, 'diversity', 'kci']\\\n",
    "                               +[f'kh_{i}' for i in range(1, 20+1)]]\\\n",
    "                               .drop_duplicates(keep='first'), \n",
    "                           on=[f'{ar}_{year_style}_period', region_corporation], \n",
    "                           how='inner')\n",
    "# for period in right_person_df[f'{ar}_{year_style}_period'].unique():\n",
    "#     right_person_df\n",
    "\n",
    "# for period in right_person_df[f'{ar}_{year_style}_period'].unique():\n",
    "#     for i in range(1, 20+1):\n",
    "#         value = right_person_df[right_person_df[f'{ar}_{year_style}_period']==period]\n",
    "#         right_person_df[right_person_df[f'{ar}_{year_style}_period']==period][f'kh_{i}'] = (value[f'kh_{i}'] - value[f'kh_{i}'].mean()) / value[f'kh_{i}'].std()\n",
    "#     display(right_person_df[right_person_df[f'{ar}_{year_style}_period'] == period].head())\n",
    "#     display(right_person_df[right_person_df[f'{ar}_{year_style}_period'] == period].describe())\n",
    "#     print(right_person_df[right_person_df[f'{ar}_{year_style}_period'] == period].info())\n",
    "#     print('\\n')\n",
    "# right_person_df['reg_num'] = right_person_df['reg_num'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_person_df.to_csv(f'{output_dir}corporations/{output_condition}.csv', \n",
    "                       encoding='utf-8', \n",
    "                       sep=',', \n",
    "                       index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right_person_df.to_excel('../../output/tables/KCI.xlsx', \n",
    "#                          index=False, \n",
    "#                          sheet_name=output_condition)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>先頭に戻る</a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=ipc></a>\n",
    "\n",
    "### **5.2. IPC**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各期間\n",
    "classification_df = pd.merge(c_df.groupby([f'{ar}_{year_style}_period', classification])[['reg_num']].sum().reset_index(drop=False), \n",
    "                        c_df.groupby([f'{ar}_{year_style}_period', classification])[[region_corporation]].nunique().reset_index(drop=False), \n",
    "                        on=[f'{ar}_{year_style}_period', classification], \n",
    "                        how='inner')\n",
    "classification_df = pd.merge(classification_df, \n",
    "                      c_df[[f'{ar}_{year_style}_period', classification, 'ubiquity', 'tci']\\\n",
    "                          +[f'ki_{i}' for i in range(1, 20+1)]]\\\n",
    "                          .drop_duplicates(keep='first'), \n",
    "                      on=[f'{ar}_{year_style}_period', classification], \n",
    "                      how='inner')\n",
    "\n",
    "# classification_df['reg_num'] = classification_df['reg_num'].astype(np.int64)\n",
    "# classification_df = pd.merge(classification_df, \n",
    "#                             schmoch_df.rename(columns={'Field_number':classification}), \n",
    "#                             on=[classification], \n",
    "#                             how='inner')\\\n",
    "#                             .drop(columns=[classification])\\\n",
    "#                             .rename(columns={'Field_en':classification})\n",
    "# display(classification_df)\n",
    "schmoch_df['ipc3'] = schmoch_df['IPC_code'].str[:3]\n",
    "schmoch_df = schmoch_df.drop_duplicates()\n",
    "schmoch_df\n",
    "classification_df = pd.merge(classification_df,\n",
    "                                schmoch_df,\n",
    "                                on=['ipc3'],\n",
    "                                how='left')\\\n",
    "                            .rename(columns={'Field_en':'schmoch35'})\\\n",
    "                            .drop(columns=['IPC_code', 'Field_number'])\\\n",
    "                            .drop_duplicates()\n",
    "# for period in classification_df[f'{ar}_{year_style}_period'].unique():\n",
    "#     classification_df[classification_df[f'{ar}_{year_style}_period']==period]['tci'] = (classification_df[classification_df[f'{ar}_{year_style}_period']==period]['tci'] - classification_df[classification_df[f'{ar}_{year_style}_period']==period]['tci'].min()) / (classification_df[classification_df[f'{ar}_{year_style}_period']==period]['tci'].max() - classification_df[classification_df[f'{ar}_{year_style}_period']==period]['tci'].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.to_csv(f'{output_dir}tech/{output_condition}.csv', \n",
    "                        encoding='utf-8', \n",
    "                        sep=',', \n",
    "                        index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_df[classification_df[f'{ar}_{year_style}_period']==f'{year_start}-{year_end}']\\\n",
    "#     [['schmoch35', 'reg_num', 'ubiquity', 'tci']]\\\n",
    "#     .rename(columns={'reg_num':'patent count', 'ubiquity':'degree centrality', 'tci':'TCI'})\\\n",
    "#     .to_excel('../../output/tables/TCI.xlsx', \n",
    "#                          index=False, \n",
    "#                          sheet_name=output_condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openpyxl.Workbook()\n",
    "# import openpyxl as xl\n",
    "# from openpyxl.styles.borders import Border, Side\n",
    "\n",
    "# wb1 = xl.load_workbook(filename='../../output/tables/TCI.xlsx')\n",
    "# ws1 = wb1[output_condition]\n",
    "# side = Side(style='thick', color='000000')\n",
    "\n",
    "# border = Border(top=side, bottom=side, left=side, right=side)\n",
    "\n",
    "# for row in ws1:\n",
    "#     for cell in row:\n",
    "#         ws1[cell.coordinate].border = border\n",
    "# wb1.save('../../output/tables/TCI.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=#top>先頭に戻る</a>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=network></a>\n",
    "\n",
    "## **5.3. 二部グラフ用**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eneos_df = c_df[(c_df[f'{ar}_{year_style}_period']==f'{year_start}-{year_end}')&(c_df[region_corporation].str.contains('ＥＮＥＯＳ'))\\\n",
    "#                 &(c_df['mcp']==1)].copy()#[[region_corporation, 'reg_num', 'schmoch35']].copy()\n",
    "# eneos_df = pd.merge(eneos_df, \n",
    "#                     schmoch_df.rename(columns={'Field_number':'schmoch35'})\\\n",
    "#                               .drop_duplicates(keep='first'), \n",
    "#                     on=['schmoch35'], \n",
    "#                     how='inner')\n",
    "# eneos_df[['ubiquity', 'Field_en', 'ki_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df[classification_df[f'{ar}_{year_style}_period']==f'{year_start}-{year_end}']\\\n",
    "    [['schmoch35', 'reg_num', 'ubiquity', 'tci']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df = classification_df[classification_df[f'{ar}_{year_style}_period']==f'{year_start}-{year_end}']\\\n",
    "                    [['schmoch35', 'reg_num', 'ubiquity', 'tci']].copy()\n",
    "schmoch5_df = pd.read_csv(f'{ex_dir}35.csv', \n",
    "                          encoding='utf-8', \n",
    "                          sep=',', \n",
    "                          usecols=['Field_number', 'Field_en', 'schmoch5']\n",
    "                          ).drop_duplicates(ignore_index=True)\n",
    "ps_df = pd.merge(ps_df, schmoch5_df, left_on='schmoch35', right_on='Field_en', how='inner')\\\n",
    "            .drop(columns=['schmoch35'])\\\n",
    "            .rename(columns={'Field_en':'label', 'Field_number':'node_id'})\n",
    "ps_df['tci'] = (ps_df['tci'] - ps_df['tci'].min()) / (ps_df['tci'].max() - ps_df['tci'].min()) * 100\n",
    "ps_df['node_id'] -= 1\n",
    "ps_df[['node_id', 'label', 'reg_num', 'ubiquity', 'tci', 'schmoch5']].to_csv(f'{output_dir}product_space/{output_condition}_node.tsv', \n",
    "            sep='\\t',\n",
    "            encoding='utf-8', \n",
    "            index=False)\n",
    "ps_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_df[['node_id', 'label', 'reg_num', 'ubiquity', 'tci', 'schmoch5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_edge_df = prox_df[prox_df[f'{ar}_{year_style}_period']==f'{year_start}-{year_end}']\\\n",
    "                    .rename(columns={'schmoch35_1':'source', 'schmoch35_2':'target', 'proximity':'weight'})\\\n",
    "                    .drop(columns=[f'{ar}_{year_style}_period'])\\\n",
    "                    .drop_duplicates(subset=['source', 'target'], ignore_index=True)\n",
    "ps_edge_df['source'] -= 1\n",
    "ps_edge_df['target'] -= 1\n",
    "ps_edge_df.to_csv(f'{output_dir}product_space/{output_condition}_edge.tsv', \n",
    "            sep='\\t',\n",
    "            encoding='utf-8', \n",
    "            index=False)\n",
    "ps_edge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = pd.merge(c_df, schmoch_df, \n",
    "                    left_on=classification, right_on='Field_number', how='left')\\\n",
    "                    .drop(columns=['Field_number', classification])\\\n",
    "                    .rename(columns={'Field_en':classification})\n",
    "# graph_df = graph_df[graph_df['mcp']==1][[f'{ar}_{year_style}', region_corporation, 'ipc_class', 'mcp']]\n",
    "graph_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edge_df = graph_df[(graph_df[f'{ar}_{year_style}_period'] == f'{year_start}-{year_end}')&(graph_df['mcp'] == 1)].copy()\\\n",
    "                      [[region_corporation, classification, 'mcp']].rename(columns={'mcp':'Weight'})\n",
    "all_edge_df['Type'] = 'Undirected'\n",
    "all_edge_df\n",
    "\n",
    "all_node_list = list(all_edge_df[region_corporation].unique()) + list(all_edge_df[classification].unique())\n",
    "all_flag_list = [0] * len(all_edge_df[region_corporation].unique()) + [1] * len(all_edge_df[classification].unique())\n",
    "all_node_df = pd.DataFrame(all_node_list, columns=['label']).reset_index(drop=False).rename(columns={'index':'node_id'})\n",
    "all_node_df['projected'] = all_flag_list\n",
    "all_node_df['node_id'] += 1\n",
    "\n",
    "all_edge_df = pd.merge(all_edge_df, all_node_df, left_on=region_corporation, right_on='label', how='left').rename(columns={'node_id':'Source'})\n",
    "all_edge_df = pd.merge(all_edge_df, all_node_df, left_on=classification, right_on='label', how='left').rename(columns={'node_id':'Target'})\n",
    "\n",
    "all_edge_df = all_edge_df[['Source', 'Target', 'Type', 'Weight']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_node_df.to_csv(f'{output_dir}graph/{output_condition}_node.csv', \n",
    "                     encoding='utf-8', \n",
    "                     sep=',', \n",
    "                     index=False)\n",
    "all_edge_df.to_csv(f'{output_dir}graph/{output_condition}_edge.csv',\n",
    "                     encoding='utf-8',\n",
    "                     sep=',',\n",
    "                     index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph_df.to_csv(f'{output_dir}graph/{output_condition}.csv', \n",
    "#                 encoding='utf-8', \n",
    "#                 sep=',', \n",
    "#                 index=False)\n",
    "# graph_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economic_complexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
