{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "from ecomplexity import ecomplexity\n",
    "\n",
    "# 小数点以下 桁数 6\n",
    "pd.options.display.float_format = '{:.3f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import initial_condition\n",
    "from process import weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir, filter_dir, output_dir\n",
    "data_dir = '../../data/interim/internal/filtered_before_agg/'\n",
    "filter_dir = '../../data/interim/internal/filter_after_agg/'\n",
    "output_dir = '../../data/interim/internal/filtered_after_agg/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期条件\n",
    "ar = initial_condition.AR\n",
    "year_style = initial_condition.YEAR_STYLE\n",
    "\n",
    "year_start = initial_condition.YEAR_START\n",
    "year_end = initial_condition.YEAR_END\n",
    "year_range = initial_condition.YEAR_RANGE\n",
    "year_range = 5\n",
    "\n",
    "extract_population = initial_condition.EXTRACT_POPULATION\n",
    "top_p_or_num = initial_condition.TOP_P_OR_NUM\n",
    "top_p_or_num = ('p', 3)\n",
    "region_corporation = initial_condition.REGION_CORPORATION\n",
    "region_corporation = 'right_person_name'\n",
    "applicant_weight = initial_condition.APPLICANT_WEIGHT\n",
    "\n",
    "classification = initial_condition.CLASSIFICATION\n",
    "# classification = 'ipc'\n",
    "ipc_digit = 3\n",
    "class_weight = initial_condition.CLASS_WEIGHT\n",
    "\n",
    "filter_condition = f'{ar}_{year_style}_{extract_population}_reg_num_top_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}'\n",
    "output_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全体\n",
    "all_df = pd.read_csv(f'{data_dir}japan.csv', \n",
    "                     encoding='utf-8', \n",
    "                     sep=',', \n",
    "                     usecols=['reg_num', \n",
    "                              region_corporation, \n",
    "                              f'{ar}_{year_style}', \n",
    "                              f'{classification}'], \n",
    "                     dtype={'reg_num': str, \n",
    "                            region_corporation: str, \n",
    "                            f'{ar}_{year_style}': np.int64, \n",
    "                            f'{classification}': str})\n",
    "\n",
    "# all_df[f'ipc{ipc_digit}'] = all_df['ipc'].str[:ipc_digit]\n",
    "all_df = all_df[all_df[f'{ar}_{year_style}'].isin(range(year_start, year_end+1))]\\\n",
    "    .drop_duplicates()\n",
    "            #    .drop(columns=['ipc'])\\\n",
    "            #    .drop_duplicates()\n",
    "# classification = f'ipc{ipc_dig/it}'\n",
    "display(all_df.head())\n",
    "\n",
    "\n",
    "# 各期間\n",
    "sep_year_df_dict = {}\n",
    "\n",
    "for year in range(year_start, year_end+1, year_range):\n",
    "    sep_year_df_dict[f'{year}-{year+year_range-1}'] = all_df[all_df[f'{ar}_{year_style}'].isin(range(year, year+year_range))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['right_person_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特許分類による重みづけ\n",
    "# 全体\n",
    "if class_weight == 'fraction':\n",
    "    all_df = weight.by_classification(all_df, region_corporation, classification)\n",
    "elif class_weight == 'duplication':\n",
    "    all_df['class_weight'] = 1\n",
    "all_df[f'{ar}_{year_style}_period'] = f'{year_start}-{year_end}'\n",
    "\n",
    "\n",
    "# 期間ごと\n",
    "# sep_year_df_dict = {}\n",
    "# sep_year_reg_num_top_df_dict = {}\n",
    "for period, sep_year_df in sep_year_df_dict.items():\n",
    "    if class_weight == 'fraction':\n",
    "        sep_year_df_dict[period] = weight.by_classification(sep_year_df, region_corporation, classification)\n",
    "    elif class_weight == 'duplication':\n",
    "        sep_year_df_dict[period] = sep_year_df.groupby([region_corporation, classification])[['reg_num']].nunique().reset_index(drop=False)\n",
    "    sep_year_df_dict[period][f'{ar}_{year_style}_period'] = period\n",
    "\n",
    "# 共同出願の重みづけ\n",
    "# 全体\n",
    "if applicant_weight == 'fraction':\n",
    "    all_df = weight.by_applicant(all_df, region_corporation)\n",
    "elif applicant_weight == 'duplication':\n",
    "    all_df['applicant_weight'] = 1\n",
    "all_df[f'{ar}_{year_style}_period'] = f'{year_start}-{year_end}'\n",
    "\n",
    "\n",
    "# 期間ごと\n",
    "# sep_year_df_dict = {}\n",
    "# sep_year_reg_num_top_df_dict = {}\n",
    "for period, sep_year_df in sep_year_df_dict.items():\n",
    "    if applicant_weight == 'fraction':\n",
    "        sep_year_df_dict[period] = weight.by_applicant(sep_year_df, region_corporation)\n",
    "    elif applicant_weight == 'duplication':\n",
    "        sep_year_df_dict[period]['applicant_weight'] = 1\n",
    "    sep_year_df_dict[period][f'{ar}_{year_style}_period'] = period\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reg_num_df = all_df.copy()\n",
    "all_reg_num_df['reg_num'] = 1 / all_reg_num_df['class_weight'] / all_reg_num_df['applicant_weight']\n",
    "all_reg_num_df = all_reg_num_df.groupby([f'{ar}_{year_style}_period', region_corporation, classification])[['reg_num']]\\\n",
    "                               .sum().reset_index(drop=False)\\\n",
    "                               .sort_values(['reg_num'], ascending=[False])\n",
    "sep_year_reg_num_df_dict = sep_year_df_dict.copy()\n",
    "for period, sep_year_reg_num_df in sep_year_reg_num_df_dict.items():\n",
    "    sep_year_reg_num_df['reg_num'] = 1 / sep_year_reg_num_df['class_weight'] / sep_year_reg_num_df['applicant_weight']\n",
    "    sep_year_reg_num_df = sep_year_reg_num_df.groupby([f'{ar}_{year_style}_period', region_corporation, classification])[['reg_num']]\\\n",
    "                                             .sum().reset_index(drop=False)\\\n",
    "                                             .sort_values(['reg_num'], ascending=[False])\n",
    "    sep_year_reg_num_df_dict[period] = sep_year_reg_num_df\n",
    "sep_year_reg_num_df = pd.concat([sep_year_reg_num_df for sep_year_reg_num_df in sep_year_reg_num_df_dict.values()], axis='index', ignore_index=True)\n",
    "sep_year_reg_num_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フィルタリング\n",
    "reg_num_filter_df = pd.read_csv(f'{filter_dir}{filter_condition}.csv',\n",
    "                                encoding='utf-8',\n",
    "                                sep=',', \n",
    "                                usecols=[f'{ar}_{year_style}_period', region_corporation],\n",
    "                                dtype=str)\n",
    "reg_num_filter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if extract_population == 'all':\n",
    "    all_reg_num_top_df = pd.merge(\n",
    "        all_reg_num_df,\n",
    "        reg_num_filter_df,\n",
    "        on=[f'{ar}_{year_style}_period', region_corporation],\n",
    "        how='inner',\n",
    "    )\n",
    "    # sep_year_reg_num_top_df = pd.merge(\n",
    "    #     sep_year_reg_num_df,\n",
    "    #     reg_num_filter_df[[region_corporation]],\n",
    "    #     on=[region_corporation], \n",
    "    #     how='inner'\n",
    "    # )\n",
    "    sep_year_reg_num_top_df = sep_year_reg_num_df[sep_year_reg_num_df[region_corporation].isin(reg_num_filter_df[region_corporation])]\n",
    "sep_year_reg_num_top_df\n",
    "\n",
    "reg_num_top_df = pd.concat([all_reg_num_top_df, sep_year_reg_num_top_df], \n",
    "                           axis='index', ignore_index=True)\n",
    "reg_num_top_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_condition = f'{ar}_{year_style}_{extract_population}_{top_p_or_num[0]}_{top_p_or_num[1]}_{region_corporation}_{applicant_weight}_{classification}_{class_weight}'\n",
    "reg_num_top_df.to_csv(f'{output_dir}{output_condition}.csv', \n",
    "                      encoding='utf-8', \n",
    "                      sep=',', \n",
    "                      index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニークID数を取得\n",
    "total_unique_ids = len(df[\"ID\"].unique())\n",
    "\n",
    "# 同じ頻度のID数を集計\n",
    "frequency_count = df[\"frequency\"].value_counts().sort_index()\n",
    "\n",
    "# 累積確率の計算\n",
    "cumulative_probability = frequency_count / total_unique_ids\n",
    "\n",
    "# 補累積確率\n",
    "complementary_cumulative_probability = []\n",
    "for i in range(len(cumulative_probability)):\n",
    "    # 補累積確率は頻度 k+1 以降の累積確率の合計\n",
    "    complementary_cumulative_probability.append(cumulative_probability[i + 1 :].sum())\n",
    "\n",
    "def ccdf(df: pd.DataFrame) -> np.ndarray:\n",
    "    freq_array = np.array(\n",
    "                        np.bincount(\n",
    "                                    df['frequency'].value_counts(ascending=True).to_dict().values()\n",
    "                                    )\n",
    "                        )\n",
    "    p_list = []\n",
    "    cumsum = 0.0\n",
    "    s = float(freq_array.sum())\n",
    "    for freq in freq_array:\n",
    "        if freq != 0:\n",
    "            cumsum += freq / s\n",
    "            p_list.append(cumsum)\n",
    "        else:\n",
    "            p_list.append(1.0)\n",
    "            \n",
    "    ccdf_array = 1 - np.array(p_list)\n",
    "    if ccdf_array[0] == 0:\n",
    "        ccdf_array[0] = 1.0\n",
    "    return ccdf_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economic_complexity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
